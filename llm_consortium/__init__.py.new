import click
import json
import llm
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import logging
import sys
import re
import os
import pathlib
import sqlite_utils
from pydantic import BaseModel
import time
import concurrent.futures
import threading

# Import strategy components
# Ensure strategies directory is treated as a package
try:
    # Use relative import assuming strategies is a subpackage
    from .strategies import ConsortiumStrategy, create_strategy, DefaultStrategy
    # Also import escape_xml utility if needed
    from llm.utils import escape_xml
except ImportError as e:
     # Provide a more informative error if strategies can't be imported
     print(f"FATAL ERROR: Could not import strategy components. Ensure 'strategies' directory and '__init__.py' exist relative to this file. Error: {e}", file=sys.stderr)
     # Define minimal fallbacks to potentially allow partial loading or clearer error messages downstream
     class ConsortiumStrategy: pass
     class DefaultStrategy(ConsortiumStrategy): pass
     def create_strategy(name, orchestrator, params): return DefaultStrategy()
     # Define escape_xml fallback if llm.utils is unavailable (shouldn't happen if llm is installed)
     try: from llm.utils import escape_xml
     except ImportError: def escape_xml(text): return text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
     # Consider exiting or raising a more specific error in a production scenario
     # raise RuntimeError("Failed to load core strategy components.") from e


# Setup logging
def _read_prompt_file(filename: str) -> str:
    """Reads a prompt file from the same directory as this script."""
    try:
        file_path = pathlib.Path(__file__).parent / filename
        with open(file_path, "r", encoding='utf-8') as f: # Specify encoding
            return f.read().strip()
    except FileNotFoundError:
         # Use logger if available, otherwise print
         log_func = logging.getLogger("llm_consortium").warning if logging.getLogger("llm_consortium").hasHandlers() else print
         log_func(f"Prompt file not found: {filename}. Using minimal fallback.")
         # Provide functional fallbacks
         if 'system' in filename: return "You are a helpful assistant."
         if 'arbiter' in filename: return "<prompt><original_prompt>{original_prompt}</original_prompt><user_instructions>{user_instructions}</user_instructions>{formatted_history}{formatted_responses}<task>Synthesize responses...</task></prompt>" # Basic structure
         if 'iteration' in filename: return "Original Prompt:\n{original_prompt}\n\nRefine based on previous attempt:\n{previous_synthesis}"
         return "" # Generic fallback
    except Exception as e:
        log_func = logging.getLogger("llm_consortium").error if logging.getLogger("llm_consortium").hasHandlers() else print
        log_func(f"Error reading {filename}: {e}")
        return "" # Generic fallback on other errors

DEFAULT_SYSTEM_PROMPT = _read_prompt_file("system_prompt.txt")
DEFAULT_ARBITER_PROMPT_TEMPLATE = _read_prompt_file("arbiter_prompt.xml")
DEFAULT_ITERATION_PROMPT_TEMPLATE = _read_prompt_file("iteration_prompt.txt")


def user_dir() -> pathlib.Path:
    """Gets the application data directory, creating it if necessary."""
    path = pathlib.Path(click.get_app_dir("io.datasette.llm"))
    path.mkdir(parents=True, exist_ok=True) # Ensure directory exists
    return path

def logs_db_path() -> pathlib.Path:
    """Gets the path to the LLM logs database."""
    return user_dir() / "logs.db"

# Global flag to prevent multiple handler additions if setup is called repeatedly
_logging_configured = False

def setup_logging() -> None:
    """Configure logging for the consortium module."""
    global _logging_configured
    # Avoid reconfiguring if already done in this process
    if _logging_configured and logging.getLogger("llm_consortium").hasHandlers():
        return

    log_path = user_dir() / "consortium.log"
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Console handler: INFO level for user feedback
    console_handler = logging.StreamHandler(sys.stderr)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)

    # File handler: DEBUG level for detailed tracing
    file_handler = None
    try:
        log_path.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(str(log_path), encoding='utf-8', mode='a') # Append mode
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
    except Exception as e:
        # Use print as logger might not be fully set up yet
        print(f"Warning: Could not setup file logging at {log_path}. Error: {e}", file=sys.stderr)

    # Get the specific logger for this module
    logger = logging.getLogger("llm_consortium")
    # Set level for the logger itself (controls what gets passed to handlers)
    logger.setLevel(logging.DEBUG)
    # Prevent messages propagating to the root logger if it has handlers (avoid duplicate console logs)
    logger.propagate = False

    # Clear existing handlers for this specific logger before adding new ones
    # This is important if setup_logging might be called multiple times (e.g., in tests)
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # Add the new handlers
    logger.addHandler(console_handler)
    if file_handler:
         logger.addHandler(file_handler)

    _logging_configured = True
    # Use the logger instance we just configured
    logger.debug("Consortium logging configured.")


# Call setup_logging when the module is loaded
setup_logging()
# Get the logger instance for use within the module
logger = logging.getLogger("llm_consortium")
logger.debug(f"llm_consortium module ({__name__}) is being imported/reloaded.")


class DatabaseConnection:
    """Manages thread-local connections to the LLM logs database."""
    _thread_local = threading.local()

    @classmethod
    def get_connection(cls) -> sqlite_utils.Database:
        """Gets or creates a thread-local database connection."""
        db = getattr(cls._thread_local, 'db', None)
        if db is None:
            try:
                db_path = logs_db_path()
                logger.info(f"Initializing database connection for thread {threading.get_ident()} at {db_path}")
                db_path.parent.mkdir(parents=True, exist_ok=True)
                db = sqlite_utils.Database(db_path)
                cls._thread_local.db = db # Store the connection
                logger.info(f"Database connection successful for thread {threading.get_ident()}")
            except Exception as e:
                logger.exception(f"Failed to initialize database connection for thread {threading.get_ident()}: {e}")
                cls._thread_local.db = None # Ensure db is None on failure
                raise # Propagate the error
        # Simple check if connection might be closed (more robust checks could ping DB)
        try:
            if db: db.path
        except Exception as e:
             logger.warning(f"DB connection check failed ({e}). Re-initializing.")
             cls._thread_local.db = None
             return cls.get_connection() # Recursive call to re-init
        return db

def _get_finish_reason(response_json: Dict[str, Any]) -> Optional[str]:
    """Extracts and normalizes the finish reason from various LLM response formats."""
    if not isinstance(response_json, dict): return None
    reason_keys = ['finish_reason', 'finishReason', 'stop_reason', 'completionReason', 'terminationReason']
    lower_response = {str(k).lower(): v for k, v in response_json.items()}
    for key in reason_keys:
        value = lower_response.get(key.lower())
        if value is not None:
            norm_val = str(value).lower().replace("_", "").replace("-","")
            if "stop" in norm_val: return "stop"
            if "length" in norm_val: return "length"
            if "max" in norm_val and ("token" in norm_val or "tok" in norm_val): return "length"
            if "tool" in norm_val and "call" in norm_val: return "tool_call"
            if "function" in norm_val and "call" in norm_val: return "tool_call"
            return norm_val # Return normalized if specific cases don't match
    return None

def log_response(response, model_identifier: str):
    """Logs an LLM response to the database and checks for truncation."""
    logger.debug(f"Logging response for {model_identifier}")
    try:
        db = DatabaseConnection.get_connection()
        if hasattr(response, 'log_to_db') and callable(response.log_to_db):
             try: response.log_to_db(db); logger.debug(f"Logged via response.log_to_db for {model_identifier}")
             except Exception as log_db_err: logger.error(f"Error calling response.log_to_db for {model_identifier}: {log_db_err}")
        else: logger.warning(f"Response object for {model_identifier} lacks standard log_to_db method.")

        response_data = None
        if hasattr(response, 'response_json') and response.response_json: response_data = response.response_json
        elif hasattr(response, 'json') and callable(response.json):
             try: response_data = response.json()
             except Exception as json_exc: logger.warning(f"Could not get JSON via response.json() for {model_identifier}: {json_exc}")

        if isinstance(response_data, dict):
            finish_reason = _get_finish_reason(response_data)
            if finish_reason == "length": logger.warning(f"Response from {model_identifier} potentially truncated (reason: {finish_reason}).")
            elif finish_reason: logger.debug(f"Finish reason for {model_identifier}: '{finish_reason}'")
            else: logger.debug(f"No clear finish reason found for {model_identifier}.")
        else: logger.debug(f"No response JSON found for {model_identifier}.")
    except sqlite_utils.db.OperationalError as db_err: logger.error(f"DB locked/operational error logging {model_identifier}: {db_err}")
    except Exception as e: logger.exception(f"Unexpected error logging response for {model_identifier}: {e}")


class IterationContext:
    """Holds the results and context of a single orchestration iteration."""
    def __init__(self, iteration: int, synthesis: Dict[str, Any], model_responses: List[Dict[str, Any]]):
        self.iteration = iteration # Iteration number (1-based)
        self.synthesis = synthesis # Arbiter's output dict for this iteration
        self.model_responses = model_responses # List of raw response dicts from models (including errors)


class ConsortiumConfig(BaseModel):
    """Configuration model for the ConsortiumOrchestrator."""
    models: Dict[str, int] # Model name -> instance count mapping. E.g., {"gpt-4o-mini": 2}
    system_prompt: Optional[str] = None # If None, loads from DEFAULT_SYSTEM_PROMPT file path
    confidence_threshold: float = 0.8
    max_iterations: int = 3
    minimum_iterations: int = 1 # Must run at least this many iterations
    arbiter: str = "gemini-1.5-flash-latest" # Default arbiter model ID
    strategy_name: Optional[str] = "default" # Strategy identifier
    strategy_params: Optional[Dict[str, Any]] = {} # Parameters specific to the chosen strategy

    def get_system_prompt(self) -> str:
        """Returns the configured system prompt or the default loaded from file."""
        return self.system_prompt if self.system_prompt is not None else DEFAULT_SYSTEM_PROMPT
    def get_strategy_params(self) -> Dict[str, Any]:
        """Returns the strategy parameters, ensuring it's a dict."""
        return self.strategy_params or {}
    def to_dict(self, **kwargs) -> Dict[str, Any]:
        """Returns a dictionary representation, excluding None values by default."""
        kwargs.setdefault('exclude_none', True); return self.model_dump(**kwargs)
    @classmethod
    def from_dict(cls, data: Dict[str, Any]):
        """Creates a ConsortiumConfig instance from a dictionary, applying defaults."""
        data.setdefault('strategy_name', 'default'); data.setdefault('strategy_params', {})
        data.setdefault('arbiter', "gemini-1.5-flash-latest")
        try: return cls(**data)
        except Exception as e: logger.error(f"Failed to create ConsortiumConfig from dict: {data}. Error: {e}"); raise


class ConsortiumOrchestrator:
    """Orchestrates interaction with multiple LLMs based on a configuration and strategy."""
    def __init__(self, config: ConsortiumConfig):
        self.config = config
        logger.info(f"Initializing Orchestrator. Strategy: '{config.strategy_name or 'default'}'. Arbiter: '{config.arbiter}'. Models: {config.models}")
        self.iteration_history: List[IterationContext] = []
        self.conversation_ids: Dict[str, Optional[str]] = {}
        self.strategy: ConsortiumStrategy = self._initialize_strategy()

    def _initialize_strategy(self) -> ConsortiumStrategy:
         """Safely initializes the strategy."""
         name = self.config.strategy_name or 'default'; params = self.config.get_strategy_params()
         try: strat = create_strategy(name, self, params); logger.info(f"Initialized strategy '{name}'"); return strat
         except ValueError as e: logger.error(f"Init strategy '{name}' failed: {e}. Falling back to Default."); return DefaultStrategy(self, {})
         except Exception as e: logger.exception(f"Unexpected error init strategy '{name}'. Fallback to Default."); return DefaultStrategy(self, {})

    def orchestrate(self, prompt: str) -> Dict[str, Any]:
        """Executes the main orchestration workflow."""
        start_time = time.time(); original_prompt = prompt
        logger.info(f"--- Orchestration START. Prompt: '{original_prompt[:100]}...' ---")
        self.iteration_history = []; self.conversation_ids = {}; iteration_count = 0; final_synthesis = None
        try: self.strategy.initialize_state(); logger.debug("Strategy state initialized.")
        except Exception as e: logger.exception("Error initializing strategy state; proceeding.")

        sys_prompt = self.config.get_system_prompt()
        current_prompt_content = f"[SYSTEM]\n{sys_prompt}\n[/SYSTEM]\n\n{original_prompt}" if sys_prompt else original_prompt

        while iteration_count < self.config.max_iterations:
            iteration_count += 1; iter_start = time.time()
            logger.info(f"--- Iteration {iteration_count}/{self.config.max_iterations} START ---")

            model_prompt = f"<prompt><instruction>{escape_xml(current_prompt_content)}</instruction></prompt>" if iteration_count == 1 else current_prompt_content
            logger.debug(f"Iter {iteration_count} model prompt:\n{model_prompt[:500]}...")

            selected_models = self._execute_strategy_select(model_prompt, iteration_count)
            if selected_models is None: logger.warning("Strategy signaled stop. Ending."); break
            if not selected_models: # Empty dict means skip this iteration's processing
                logger.warning("Strategy selected no models. Skipping iteration processing.")
                empty_synth = self._create_empty_synthesis("No models selected by strategy.")
                iter_ctx = IterationContext(iteration=iteration_count, synthesis=empty_synth, model_responses=[])
                self.iteration_history.append(iter_ctx); self._execute_strategy_update(iter_ctx)
                continue # Skip to next iteration

            model_responses = self._get_model_responses(selected_models, model_prompt, iteration_count)
            successful_responses = [r for r in model_responses if 'error' not in r]
            if len(successful_responses) < len(model_responses): logger.warning(f"Iter {iteration_count}: {len(model_responses) - len(successful_responses)} model errors.")

            processed_responses = self._execute_strategy_process(successful_responses, iteration_count)
            if processed_responses is None: logger.error("Strategy processing failed. Ending."); break

            synthesis_result = self._execute_synthesis(original_prompt, processed_responses, iteration_count)
            if synthesis_result is None: logger.error("Synthesis failed. Ending."); break

            iter_ctx = IterationContext(iteration=iteration_count, synthesis=synthesis_result, model_responses=model_responses)
            self.iteration_history.append(iter_ctx); self._execute_strategy_update(iter_ctx)

            should_stop, reason = self._check_termination_conditions(synthesis_result, iteration_count)
            if should_stop: logger.info(f"Stopping: {reason}."); final_synthesis = synthesis_result; break

            current_prompt_content = self._execute_next_prompt_construction(original_prompt, synthesis_result)
            if current_prompt_content is None: logger.error("Next prompt construction failed. Stopping."); break

            logger.info(f"--- Iteration {iteration_count} END ({(time.time() - iter_start):.2f}s) ---")
        else: # Loop finished normally (max iterations reached)
             logger.info(f"Max iterations ({self.config.max_iterations}) reached.")
             if self.iteration_history: final_synthesis = self.iteration_history[-1].synthesis
             else: final_synthesis = self._create_empty_synthesis("No iterations run.")

        if final_synthesis is None: # Loop broken early, potentially without a final result
            logger.warning("Orchestration ended prematurely without final synthesis.")
            if self.iteration_history: final_synthesis = self.iteration_history[-1].synthesis
            else: final_synthesis = self._create_empty_synthesis("No iterations completed.")

        total_duration = time.time() - start_time
        logger.info(f"--- Orchestration END. Duration: {total_duration:.2f}s. Iterations: {iteration_count} ---")
        return self._construct_final_output(original_prompt, final_synthesis, iteration_count, total_duration)

    def _execute_strategy_select(self, prompt: str, iter_num: int) -> Optional[Dict[str, int]]:
        try:
            selected = self.strategy.select_models(self.config.models, prompt, iter_num)
            logger.info(f"Iter {iter_num}: Strategy selected: {selected or 'None'}")
            if not selected:
                if iter_num >= self.config.minimum_iterations: logger.warning("Strategy selected None after min iterations."); return None # Signal stop
                else: logger.warning("Strategy selected None, skipping iter processing."); return {} # Skip iter
            return selected
        except Exception as e: logger.exception(f"Critical error in strategy.select_models (Iter {iter_num}). Fallback: All models."); return self.config.models
    def _execute_strategy_process(self, responses: List[Dict[str, Any]], iter_num: int) -> Optional[List[Dict[str, Any]]]:
        try: processed = self.strategy.process_responses(responses, iter_num); logger.info(f"Iter {iter_num}: Strategy processed {len(responses)}->{len(processed)} responses."); return processed
        except Exception as e: logger.exception(f"Critical error in strategy.process_responses (Iter {iter_num}). Fallback: Unprocessed."); return responses
    def _execute_strategy_update(self, iter_ctx: IterationContext):
        try: self.strategy.update_state(iter_ctx); logger.debug(f"Strategy state updated after iter {iter_ctx.iteration}.")
        except Exception as e: logger.exception(f"Error updating strategy state after iter {iter_ctx.iteration}.")
    def _execute_synthesis(self, orig_prompt: str, proc_responses: List[Dict[str, Any]], iter_num: int) -> Optional[Dict[str, Any]]:
        if not proc_responses: logger.warning(f"Iter {iter_num}: No responses to synthesize."); return self._create_empty_synthesis("No responses for synthesis.")
        try: synth = self._synthesize_responses(orig_prompt, proc_responses, iter_num); logger.info(f"Iter {iter_num}: Synthesis done. Confidence={synth.get('confidence', -1):.2f}"); return synth
        except Exception as e: logger.exception(f"Critical error during synthesis (Iter {iter_num})."); return None
    def _execute_next_prompt_construction(self, orig_prompt: str, last_synth: Dict[str, Any]) -> Optional[str]:
         try: next_prompt = self._construct_iteration_prompt(orig_prompt, last_synth); logger.debug("Constructed next prompt."); return next_prompt
         except Exception as e: logger.exception("Failed to construct next prompt."); return None
    def _check_termination_conditions(self, synth: Dict[str, Any], iter_num: int) -> Tuple[bool, str]:
        conf = synth.get("confidence", 0.0); needs_iter = synth.get("needs_iteration", True)
        met_min = iter_num >= self.config.minimum_iterations; met_conf = conf >= self.config.confidence_threshold
        if met_conf and met_min: return True, f"Confidence met ({conf:.2f}) after min iterations ({iter_num})."
        if not needs_iter and met_min: return True, f"Arbiter needs no iteration after min iterations ({iter_num})."
        return False, "Termination conditions not met."
    def _construct_final_output(self, prompt: str, final_synth: Dict[str, Any], iter_count: int, duration: float) -> Dict[str, Any]:
         return {"original_prompt": prompt, "final_synthesis": final_synth,
                 "iteration_history": [{"iteration": ctx.iteration, "model_responses": ctx.model_responses, "synthesis": ctx.synthesis} for ctx in self.iteration_history],
                 "metadata": {"config_used": self.config.to_dict(), "strategy_used": self.config.strategy_name or 'default', "strategy_params": self.config.get_strategy_params(),
                              "arbiter_used": self.config.arbiter, "timestamp_utc": datetime.utcnow().isoformat() + "Z", "total_iterations": iter_count, "duration_seconds": round(duration, 2)}}
    def _create_empty_synthesis(self, reason: str) -> Dict[str, Any]:
         logger.warning(f"Creating empty synthesis: {reason}")
         return {"synthesis": f"Error: {reason}", "confidence": 0.0, "analysis": f"Error: {reason}", "dissent": "", "needs_iteration": False, "refinement_areas": []}
    def _get_model_responses(self, selected: Dict[str, int], prompt: str, iter_num: int) -> List[Dict[str, Any]]:
        responses = []; future_info: Dict[concurrent.futures.Future, Dict] = {}; num = sum(selected.values())
        if num == 0: logger.warning(f"Iter {iter_num}: No models selected."); return []
        workers = min(max(num, 1), 20); logger.debug(f"Iter {iter_num}: Submitting {num} tasks (workers={workers}).")
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as pool:
                for name, count in selected.items():
                    for i in range(count): key = f"{name}-{i}"; fut = pool.submit(self._get_model_response, name, prompt, i, key); future_info[fut] = {"m": name, "i": i + 1}
                for fut in concurrent.futures.as_completed(future_info):
                    info = future_info[fut]; mid = f"{info['m']}-{info['i']}"
                    try: res = fut.result(); responses.append(res); logger.debug(f"Result OK from {mid}.")
                    except Exception as e: logger.error(f"Task for {mid} failed: {e}"); responses.append({"model": info['m'], "instance": info['i'], "error": str(e), "response": "", "confidence": 0.0})
        except Exception as e: logger.exception(f"ThreadPoolExecutor failed: {e}")
        logger.debug(f"Iter {iter_num}: Collected {len(responses)} responses."); return responses
    def _get_model_response(self, name: str, prompt: str, index: int, key: str) -> Dict[str, Any]:
        mid = f"{name}-{index+1}"; logger.debug(f"Worker START for {mid}"); attempts = 0; max_retries = 3; last_err = None
        conv_id = self.conversation_ids.get(key); logger.debug(f"[{mid}] Using conv_id: {conv_id}")
        while attempts < max_retries:
            attempts += 1; t0 = time.time(); logger.debug(f"[{mid}] Attempt {attempts}/{max_retries}...")
            try:
                model = llm.get_model(name); response = model.prompt(prompt, conversation_id=conv_id)
                text = response.text(); dur = time.time() - t0; logger.debug(f"[{mid}] Response OK in {dur:.2f}s.")
                log_response(response, mid)
                new_id = getattr(response, 'conversation_id', None)
                if new_id: self.conversation_ids[key] = new_id; logger.debug(f"[{mid}] Stored new conv_id: {new_id}")
                elif conv_id: self.conversation_ids[key] = conv_id # Keep old if no new one
                else: self.conversation_ids[key] = None
                conf = self._extract_confidence(text)
                return {"model": name, "instance": index + 1, "response": text, "confidence": conf, "conversation_id": new_id}
            except Exception as e:
                last_err = e; dur = time.time() - t0; logger.warning(f"[{mid}] Attempt {attempts} failed ({dur:.2f}s): {e}")
                if attempts < max_retries: wait = 2**attempts; logger.info(f"[{mid}] Retrying in {wait}s..."); time.sleep(wait)
                else: logger.error(f"[{mid}] Max retries reached."); raise last_err
        raise last_err or RuntimeError(f"Worker {mid} failed unexpectedly.") # Should not be reached
    def _parse_confidence_value(self, text: str, default: float = 0.0) -> float:
        if not text: return default
        m = re.search(r"<confidence>\s*(\d*\.?\d+)\s*</confidence>", text, re.I | re.S)
        if m: try: v=float(m.group(1)); return v/100 if v>1 else v; except ValueError: logger.warning(f"Invalid XML confidence: {m.group(1)}"); # Fall through
        m = re.search(r"confidence\s*[:=]?\s*(\d*\.?\d+)\s*%?", text, re.I | re.S)
        if m: try: v=float(m.group(1)); return v/100 if v>1 else v; except ValueError: logger.warning(f"Invalid keyword confidence: {m.group(1)}"); # Fall through
        logger.debug(f"Confidence not found: '{text[:100]}...'"); return default
    def _extract_confidence(self, text: str) -> float: return self._parse_confidence_value(text, default=0.0)
    def _construct_iteration_prompt(self, orig_prompt: str, last_synth: Dict[str, Any]) -> str:
        tmpl = DEFAULT_ITERATION_PROMPT_TEMPLATE
        if not tmpl: logger.error("Iter template missing."); return f"Refine (template missing):\n{json.dumps(last_synth, indent=2)}"
        try:
            ctx = {k: last_synth.get(k, 'N/A') for k in ['synthesis', 'analysis', 'dissent']}
            ctx['confidence']=last_synth.get('confidence', 0.0); ctx['needs_iteration']=last_synth.get('needs_iteration', True)
            ctx['refinement_areas']=last_synth.get('refinement_areas', []); fmt_json = json.dumps(ctx, indent=2)
            instr = self.config.get_system_prompt()
            fmt_dict = {"original_prompt": orig_prompt, "previous_synthesis": fmt_json, "user_instructions": instr,
                        "refinement_areas": "\n".join(f"- {a}" for a in ctx['refinement_areas']) if ctx['refinement_areas'] else "None."}
            return tmpl.format(**fmt_dict)
        except KeyError as e: logger.error(f"KeyError in iter template: '{e}'. Fallback."); return f"Refine (template key error):\n{json.dumps(last_synth, indent=2)}"
        except Exception as e: logger.exception(f"Error constructing iter prompt: {e}. Fallback."); return f"Refine (error):\n{json.dumps(last_synth, indent=2)}"
    def _format_iteration_history(self) -> str:
        if not self.iteration_history: return "<history>No previous iterations.</history>"
        entries = []
        for ctx in self.iteration_history:
            r_xml = "\n".join(f' <mr m="{escape_xml(r.get("m", "?"))}" i="{r.get("i", 0)}"><r>{escape_xml(r.get("r", "Err:"+r.get("e","?")))}</r><c>{escape_xml(str(r.get("c", "N/A")))}</c></mr>'
                              .replace('mr','model_response').replace('m=','model=').replace('i=','instance=').replace('r>','response>').replace('c>','confidence>') # Shortenings for readability
                           for r in ctx.model_responses)
            s = ctx.synthesis; a_xml = self._format_refinement_areas(s.get("refinement_areas", []))
            entries.append(f'\n<iter n="{ctx.iteration}"><mrs>{r_xml}</mrs><syn><c>{escape_xml(s.get("s", "?"))}</c><conf>{s.get("c", 0):.2f}</conf><a>{escape_xml(s.get("a", "?"))}</a><d>{escape_xml(s.get("d", "?"))}</d><ni>{str(s.get("ni", True)).lower()}</ni><ra>{a_xml}</ra></syn></iter>'
                          .replace('iter','iteration').replace('n=','number=').replace('mrs','model_responses').replace('syn','synthesis').replace('c>','content>')
                          .replace('conf>','confidence>').replace('a>','analysis>').replace('d>','dissent>').replace('ni>','needs_iteration>').replace('ra>','refinement_areas'))
        return f"<history>{''.join(entries)}\n</history>"
    def _format_refinement_areas(self, areas: List[str]) -> str:
        if not areas: return "<area>None.</area>"; return "\n".join(f"<area>{escape_xml(a)}</area>" for a in areas)
    def _format_responses(self, responses: List[Dict[str, Any]]) -> str:
        if not responses: return "<current_responses>None.</current_responses>"
        r_xml = "\n".join(f'<mr m="{escape_xml(r.get("m", "?"))}" i="{r.get("i", 0)}"><r>{escape_xml(r.get("r", "Err"))}</r><c>{escape_xml(str(r.get("c", "N/A")))}</c></mr>'
                           .replace('mr','model_response').replace('m=','model=').replace('i=','instance=').replace('r>','response>').replace('c>','confidence>')
                        for r in responses)
        return f"<current_responses>{r_xml}\n</current_responses>"
    def _synthesize_responses(self, orig_prompt: str, responses: List[Dict[str, Any]], iter_num: int) -> Dict[str, Any]:
        arb_id = self.config.arbiter; logger.info(f"Iter {iter_num}: Synthesizing {len(responses)} responses via '{arb_id}'.")
        try: arb_model = llm.get_model(arb_id)
        except llm.UnknownModelError: logger.error(f"Arbiter '{arb_id}' unknown."); return self._create_empty_synthesis(f"Arbiter '{arb_id}' unknown.")
        tmpl = DEFAULT_ARBITER_PROMPT_TEMPLATE;
        if not tmpl: logger.error("Arbiter template missing."); return self._create_empty_synthesis("Arbiter template missing.")
        hist = self._format_iteration_history(); cur = self._format_responses(responses); instr = self.config.get_system_prompt()
        try: prompt_str = tmpl.format(original_prompt=escape_xml(orig_prompt), formatted_responses=cur, formatted_history=hist, user_instructions=escape_xml(instr), iteration_number=iter_num)
        except KeyError as e: logger.error(f"KeyError in arbiter template: '{e}'. Fallback."); prompt_str = f"<prompt><orig>{escape_xml(orig_prompt)}</orig><instr>{escape_xml(instr)}</instr>{hist}{cur}<task>Synthesize...</task></prompt>"
        except Exception as e: logger.exception(f"Arbiter prompt format error: {e}"); return self._create_empty_synthesis(f"Arbiter prompt format error: {e}")
        logger.debug(f"Arbiter prompt (Iter {iter_num}):\n{prompt_str[:500]}...")
        try: response = arb_model.prompt(prompt_str); log_response(response, f"Arbiter-{arb_id}-Iter{iter_num}"); text = response.text(); logger.debug(f"Arbiter raw response:\n{text}"); return self._parse_arbiter_response(text)
        except Exception as e: logger.exception(f"Arbiter execution/parse error: {e}"); return self._create_empty_synthesis(f"Arbiter error: {e}")
    def _parse_arbiter_response(self, text: str) -> Dict[str, Any]:
        if not text: logger.error("Empty arbiter response."); return self._create_empty_synthesis("Empty arbiter response.")
        res = {}; patterns = {"s": r"<synthesis>([\s\S]*?)</synthesis>", "c": r"<confidence>\s*(\d*\.?\d+)\s*</confidence>", "a": r"<analysis>([\s\S]*?)</analysis>",
                           "d": r"<dissent>([\s\S]*?)</dissent>", "ni": r"<needs_iteration>\s*(true|false)\s*</needs_iteration>", "ra": r"<refinement_areas>([\s\S]*?)</refinement_areas>"}
        key_map = {"s":"synthesis", "c":"confidence", "a":"analysis", "d":"dissent", "ni":"needs_iteration", "ra":"refinement_areas"}
        for k_short, pattern in patterns.items():
            k_long = key_map[k_short]; m = re.search(pattern, text, re.I | re.S)
            if m: val = m.group(1).strip()
            else: logger.warning(f"Arbiter response missing <{k_long}>."); val = None
            if k_short == "c":
                try: res[k_long] = float(val)/100 if val and float(val)>1 else float(val or 0)
                except (ValueError, TypeError): res[k_long] = 0.0; logger.warning(f"Invalid confidence '{val}'")
            elif k_short == "ni": res[k_long] = val.lower() == "true" if val is not None else True # Default true if missing
            elif k_short == "ra":
                if val: areas = re.findall(r"<area>([\s\S]*?)</area>", val, re.I | re.S); res[k_long] = [a.strip() for a in areas if a.strip()]
                else: res[k_long] = []
            else: res[k_long] = val if val is not None else f"<{k_long}> missing."
        for k_long in key_map.values(): res.setdefault(k_long, f"Error: <{k_long}> processing failed.")
        logger.debug(f"Parsed arbiter: Conf={res.get('c', -1):.2f}, NeedsIter={res.get('ni', '?')}")
        return res

# --- LLM Plugin & CLI --- (Mostly unchanged boilerplate, adapted variable names)
def parse_models(models_input: List[str], def_count: int) -> Dict[str, int]:
    # (Implementation identical to previous correct version)
    model_dict = {}
    if not models_input: return {}
    for item in models_input:
        if ':' in item:
            name, _, count_str = item.rpartition(':'); name=name.strip(); count_str=count_str.strip()
            if not name or not count_str: raise click.ClickException(f"Invalid format: '{item}'")
            try: count = int(count_str); assert count >= 1; model_dict[name] = count
            except (ValueError, AssertionError): raise click.ClickException(f"Invalid count '{count_str}' in '{item}'")
        else: name = item.strip(); assert name; model_dict[name] = def_count
    return model_dict
def read_stdin_if_not_tty() -> Optional[str]:
    # (Implementation identical)
    if not sys.stdin.isatty(): logger.debug("Reading from stdin."); return sys.stdin.read().strip()
    logger.debug("Stdin is TTY."); return None
class ConsortiumModel(llm.Model):
    # (Implementation identical, relies on self.model_id set correctly)
    can_stream = False
    class Options(llm.Options): confidence_threshold: Optional[float]=None; max_iterations: Optional[int]=None; minimum_iterations: Optional[int]=None; arbiter: Optional[str]=None; strategy_name: Optional[str]=None # Allows -o overrides
    def __init__(self, config_name: str, config: ConsortiumConfig): self.model_id = config_name; self.base_config = config; self._orchestrator_cache = {}
    def __str__(self): return f"Consortium: {self.model_id}"
    def _get_effective_config(self, opts: Optional[llm.Options]=None, sys_prompt: Optional[str]=None) -> ConsortiumConfig:
        eff_data = self.base_config.model_dump(); sp_overrides = {}
        if sys_prompt is not None: eff_data['system_prompt'] = sys_prompt
        if opts:
            opts_dict = opts.model_dump(exclude_unset=True)
            for k, v in opts_dict.items():
                if k in ConsortiumConfig.model_fields: eff_data[k] = v
                elif k.startswith("sp_") and len(k)>3: sp_overrides[k[3:]] = v
        eff_data['strategy_params'] = {**eff_data.get('strategy_params', {}), **sp_overrides}
        try: return ConsortiumConfig.from_dict(eff_data)
        except Exception as e: raise ValueError(f"Invalid config overrides for {self.model_id}: {e}")
    def execute(self, prompt: llm.Prompt, stream: bool, response: llm.Response, conversation):
        if stream: logger.warning(f"Consortium '{self.model_id}' doesn't stream.")
        try:
            eff_cfg = self._get_effective_config(prompt.options, prompt.system); cfg_key = eff_cfg.model_dump_json()
            orch = self._orchestrator_cache.get(cfg_key)
            if not orch: logger.info(f"Creating orchestrator for {self.model_id}"); orch = ConsortiumOrchestrator(eff_cfg); self._orchestrator_cache[cfg_key] = orch
            result = orch.orchestrate(prompt.prompt); response.response_json = result
            yield result.get("final_synthesis", {}).get("synthesis", "Error.")
        except Exception as e: logger.exception(f"Consortium {self.model_id} failed."); raise llm.ModelError(f"Consortium '{self.model_id}' failed: {e}")

CONSORTIUM_CONFIG_TABLE = "llm_consortium_configs" # DB table name
def _ensure_config_table_exists(db: sqlite_utils.Database):
    # (Implementation identical)
    if CONSORTIUM_CONFIG_TABLE not in db.table_names():
        logger.info(f"Creating table '{CONSORTIUM_CONFIG_TABLE}'"); db[CONSORTIUM_CONFIG_TABLE].create({"name":str, "config":str, "created_at":str}, pk="name")
def _get_consortium_configs() -> Dict[str, ConsortiumConfig]:
    # (Implementation identical)
    cfgs = {};
    try: db=DatabaseConnection.get_connection(); _ensure_config_table_exists(db)
    except Exception as e: logger.exception(f"DB connection/table check failed: {e}"); return cfgs # Return empty on DB error
    try:
        for row in db[CONSORTIUM_CONFIG_TABLE].rows:
            try: cfgs[row["name"]] = ConsortiumConfig.from_dict(json.loads(row["config"]))
            except Exception as e: logger.error(f"Failed load config '{row['name']}': {e}. Skipping.")
    except Exception as e: logger.exception(f"Failed reading configs table: {e}")
    return cfgs
def _save_consortium_config(name: str, config: ConsortiumConfig):
    # (Implementation identical)
    try: db=DatabaseConnection.get_connection(); _ensure_config_table_exists(db); db[CONSORTIUM_CONFIG_TABLE].insert({"name":name, "config":config.model_dump_json(indent=2), "created_at":datetime.utcnow().isoformat()+"Z"}, replace=True); logger.info(f"Saved config '{name}'.")
    except Exception as e: logger.exception(f"Failed save config '{name}': {e}"); raise
def _remove_consortium_config(name: str) -> bool:
    # (Implementation identical)
    try: db=DatabaseConnection.get_connection(); _ensure_config_table_exists(db)
    except Exception as e: logger.exception(f"DB connection/table check failed for remove: {e}"); return False
    try:
        if db[CONSORTIUM_CONFIG_TABLE].get(name): db[CONSORTIUM_CONFIG_TABLE].delete(name); logger.info(f"Removed config '{name}'."); return True
        else: logger.warning(f"Config '{name}' not found for removal."); return False
    except Exception as e: logger.exception(f"Failed remove config '{name}': {e}"); return False

from click_default_group import DefaultGroup # Assuming this utility is available
class DefaultToRunGroup(DefaultGroup): # (Implementation identical)
    def __init__(self,*a,**kw): super().__init__(*a,default_if_no_args=False,ignore_unknown_options=True,**kw); self.default_cmd_name='run'
    def resolve_command(self,ctx,args):
        try:
            if args and not args[0].startswith('-') and super().get_command(ctx, args[0]): return super().resolve_command(ctx, args)
            logger.debug(f"Defaulting to '{self.default_cmd_name}'. Args: {args}"); return super().resolve_command(ctx, [self.default_cmd_name]+args)
        except click.exceptions.UsageError as e:
            if self.default_cmd_name in str(e) and "No such command" in str(e): logger.error("Default cmd missing!"); click.echo("Error!",err=True); ctx.exit(1)
            raise e
common_config_options = [ # (List identical)
    click.option("-m", "--model", "models_input", multiple=True, help="Model(s). Format: 'model' or 'model:count'."),
    click.option("-n", "--count", "default_count", type=int, default=1, show_default=True, help="Default instance count."),
    click.option("--arbiter", default=ConsortiumConfig.model_fields['arbiter'].default, show_default=True, help="Arbiter model ID."),
    click.option("--confidence-threshold", type=click.FloatRange(0.0,1.0), default=ConsortiumConfig.model_fields['confidence_threshold'].default, show_default=True, help="Min confidence (0.0-1.0)."),
    click.option("--max-iterations", type=int, default=ConsortiumConfig.model_fields['max_iterations'].default, show_default=True, help="Max iterations."),
    click.option("--min-iterations", type=int, default=ConsortiumConfig.model_fields['minimum_iterations'].default, show_default=True, help="Min iterations."),
    click.option("--system", help="System prompt (overrides default/saved)."),
    click.option("--strategy", "strategy_name", default=ConsortiumConfig.model_fields['strategy_name'].default, show_default=True, help="Strategy name."),
    click.option("--strategy-param", "strategy_params_opts", multiple=True, type=str, help="Strategy param key=value."), ]
def add_options(opts): # (Implementation identical)
    def decorator(f):
        for opt in reversed(opts): f = opt(f)
        return f
    return decorator
def parse_strategy_params(opts: Tuple[str]) -> Dict[str, Any]: # (Implementation identical)
    params = {}
    for opt in opts:
        if '=' not in opt: raise click.BadParameter(f"'{opt}' needs key=value", param_hint='--strategy-param')
        k, v = opt.split('=', 1); k = k.strip(); v = v.strip()
        if not k: raise click.BadParameter("Key empty", param_hint='--strategy-param')
        try: params[k] = json.loads(v) # Parse JSON types
        except json.JSONDecodeError: params[k] = v.strip('"\'') # Store as string
    return params

@llm.hookimpl
def register_commands(cli): # (CLI structure identical)
    @cli.group(cls=DefaultToRunGroup)
    def consortium(): """Manage and run LLM consortiums."""
    @consortium.command(name="run")
    @click.argument("prompt", required=False)
    @click.option("-m","--model","models_input",multiple=True,default=["gpt-4o-mini","claude-3-haiku-20240307","gemini-1.5-flash-latest"], show_default=True, help="Model(s) to include.")
    @add_options(common_config_options[1:])
    @click.option("-o","--output",type=click.Path(dir_okay=False,writable=True,path_type=pathlib.Path),help="Save JSON results.")
    @click.option("--raw",is_flag=True,help="Output only raw synthesis.")
    @click.option("-v","--verbose",is_flag=True,help="Include details in output.")
    @click.option("--stdin/--no-stdin","read_stdin",default=True,help="Read prompt from stdin if no argument.")
    def run_cmd(**kwargs):
        prompt=kwargs.pop('prompt'); read_stdin=kwargs.pop('read_stdin'); output=kwargs.pop('output'); raw=kwargs.pop('raw'); verbose=kwargs.pop('verbose')
        cfg_args=kwargs; # Remaining are config args
        if not prompt and read_stdin: prompt = read_stdin_if_not_tty()
        if not prompt: raise click.UsageError("No prompt provided.")
        try:
            models_in=cfg_args.pop('models_input'); def_count=cfg_args.pop('default_count'); strat_opts=cfg_args.pop('strategy_params_opts')
            model_dict = parse_models(models_in, def_count)
            if not model_dict: # Use defaults if -m wasn't used
                ctx=click.get_current_context(); defaults=next((p.default for p in ctx.command.params if p.name=='models_input'),[]); model_dict=parse_models(defaults,def_count); logger.info(f"Using default models: {model_dict}")
            strat_params = parse_strategy_params(strat_opts)
            config = ConsortiumConfig(models=model_dict, strategy_params=strat_params, **cfg_args)
            logger.info(f"Executing run: {config.to_dict()}")
            orch=ConsortiumOrchestrator(config); result=orch.orchestrate(prompt)
            if output: try: output.parent.mkdir(parents=True,exist_ok=True); output.write_text(json.dumps(result,indent=2,ensure_ascii=False), encoding='utf-8'); click.echo(f"Results saved: {output}",err=True) except Exception as e: click.echo(f"Error saving: {e}",err=True)
            final=result.get("final_synthesis",{}); text=final.get("synthesis","Error.")
            if raw: click.echo(text)
            else:
                click.echo("--- Final Synthesis ---"); click.echo(text)
                if verbose: click.echo(f"\nConfidence: {final.get('confidence',-1):.2f}, Analysis: {final.get('analysis','N/A')}, Iterations: {result.get('metadata',{}).get('total_iterations','?')}")
        except Exception as e: logger.exception("Run failed."); raise click.ClickException(f"{e.__class__.__name__}: {e}")
    @consortium.command(name="save")
    @click.argument("name")
    @click.option("-m","--model","models_input",multiple=True,required=True,help="Model(s). Required.")
    @add_options(common_config_options[1:])
    def save_cmd(name, models_input, **kwargs):
        if not name.strip(): raise click.UsageError("Name empty.")
        try:
            def_count=kwargs.pop('default_count'); strat_opts=kwargs.pop('strategy_params_opts')
            model_dict=parse_models(models_input, def_count); assert model_dict, "Need models"
            strat_params=parse_strategy_params(strat_opts); config=ConsortiumConfig(models=model_dict, strategy_params=strat_params, **kwargs)
            _save_consortium_config(name, config); click.echo(f"Consortium '{name}' saved.")
        except Exception as e: logger.exception(f"Save failed: {name}"); raise click.ClickException(f"Save failed: {e}")
    @consortium.command(name="list")
    def list_cmd():
        configs = _get_consortium_configs();
        if not configs: click.echo("No saved consortiums."); return
        click.echo("--- Saved Consortiums ---")
        for name, cfg in configs.items(): click.echo(f"\nName: {name}\n Models: {cfg.models}\n Arbiter: {cfg.arbiter}, Strategy: {cfg.strategy_name or 'default'}") # Basic info
    @consortium.command(name="remove")
    @click.argument("name")
    def remove_cmd(name):
        if not name.strip(): raise click.UsageError("Name empty.")
        if _remove_consortium_config(name): click.echo(f"Consortium '{name}' removed.")
        else: raise click.ClickException(f"Consortium '{name}' not found/failed.")

@llm.hookimpl
def register_models(register): # (Implementation identical)
    logger.debug("Registering saved consortiums."); configs = _get_consortium_configs()
    for name, config in configs.items():
        try: register(ConsortiumModel(config_name=name, config=config))
        except Exception as e: logger.error(f"Failed register '{name}': {e}")
    logger.debug(f"Registered {len(configs)} consortiums.")

__all__ = ['ConsortiumOrchestrator', 'ConsortiumConfig', 'create_consortium', 'ConsortiumStrategy', 'IterationContext']
__version__ = "0.4.0" # Version reflects strategy integration
logger.info(f"llm_consortium module v{__version__} loaded.")
